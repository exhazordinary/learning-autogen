# AutoGen Research Assistant v2.0

> Production-grade multi-agent AI research system with tool calling, token tracking, and real-time updates

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Version](https://img.shields.io/badge/version-2.0.0-green.svg)](https://github.com)

## ‚ú® Features

### ü§ñ Multi-Agent System
- **4 Specialized Agents**: Researcher, Analyst, Writer, Critic
- **Tool Calling**: Web search & calculator built-in
- **Smart Routing**: Dynamic agent selection based on task
- **Auto Speaker**: Intelligent conversation management

### üéØ Advanced Capabilities (NEW in v2.0)
- **Token Tracking**: Real-time cost estimation for all models
- **Web Search**: Agents can search the web for current information
- **Calculator**: Built-in mathematical computation
- **Enhanced Prompts**: Chain-of-thought reasoning
- **Context Management**: Automatic conversation truncation

### üåê Web Interface
- Modern React UI with dark/light theme
- Real-time WebSocket updates
- Markdown rendering with syntax highlighting
- Research history with quick access
- Export results as Markdown

### üîí Enterprise Ready
- JWT Authentication (optional)
- Rate limiting & CORS
- Sentry error tracking
- OpenTelemetry tracing
- Database indexes for performance

## üöÄ Quick Start

### Prerequisites
- Python 3.10+
- [uv](https://github.com/astral-sh/uv) package manager
- Redis (for async tasks)
- Ollama or OpenAI API key

### Installation

```bash
# 1. Clone and install
git clone <repo-url>
cd learning-autogen
uv sync

# 2. Start Redis
redis-server

# 3. Start backend
python app.py

# 4. Start frontend (optional)
cd frontend && npm install && npm run dev
```

### First Research Task

```python
from src.autogen_research import ResearchTeam

# Create team with tools enabled
team = ResearchTeam(enable_tools=True)

# Run research with token tracking
messages, stats = team.run("Explain quantum computing")

# View results
print(f"Tokens used: {stats['total_tokens']}")
print(f"Estimated cost: ${stats['estimated_cost']:.4f}")
```

## üí° Usage Examples

### Web Search

```python
team = ResearchTeam(enable_tools=True)

# Agent will automatically use web_search() for current info
messages, stats = team.run(
    "What are the latest developments in AI?"
)
```

### Calculator

```python
team = ResearchTeam(enable_tools=True)

# Agent will use calculator() for math
messages, stats = team.run(
    "Calculate ROI: revenue $500k, costs $350k"
)
```

### Dynamic Agent Routing

```python
team = ResearchTeam()

# Only activates needed agents (faster!)
messages, stats = team.run(
    "Calculate: 100 * 0.15",
    use_dynamic_routing=True
)
```

### Cost Tracking

```python
# Track token usage and costs
messages, stats = team.run("Research task")

print(f"Input tokens: {stats['input_tokens']}")
print(f"Output tokens: {stats['output_tokens']}")
print(f"Total cost: ${stats['estimated_cost']:.4f}")
```

## üîß Configuration

### Environment Variables

```bash
# Model Configuration
MODEL_TYPE=ollama              # ollama or openai
MODEL_NAME=llama3.2           # Model to use
TEMPERATURE=0.7               # Sampling temperature
MAX_ROUNDS=12                 # Max conversation rounds

# Ollama (local)
OLLAMA_BASE_URL=http://localhost:11434/v1

# OpenAI (cloud)
OPENAI_API_KEY=your-api-key
MODEL_NAME=gpt-4o-mini

# Database
DATABASE_URL=sqlite:///research.db

# Redis
REDIS_URL=redis://localhost:6379/0

# Authentication (optional)
JWT_SECRET_KEY=your-secret-key

# Observability (optional)
SENTRY_DSN=your-sentry-dsn
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
```

### Python API

```python
from src.autogen_research import ResearchTeam, Config, ModelConfig

# Custom configuration
config = Config(
    model=ModelConfig(
        model_type="ollama",
        model_name="llama3.2",
        temperature=0.7
    )
)

team = ResearchTeam(
    config=config,
    enable_tools=True  # Enable web_search and calculator
)

# Run with options
messages, stats = team.run(
    task="Your research question",
    verbose=True,                # Print progress
    use_dynamic_routing=True     # Smart agent selection
)
```

## üì° REST API

### Start Server

```bash
# Development
python app.py

# Production
gunicorn -k eventlet -w 1 app:app
```

### Endpoints

#### Research Task
```bash
POST /api/v1/research
Content-Type: application/json

{
  "task": "Explain quantum computing"
}
```

Response includes token statistics:
```json
{
  "success": true,
  "task_id": 1,
  "task": {
    "messages": [...],
    "metrics": {
      "input_tokens": 245,
      "output_tokens": 1823,
      "total_tokens": 2068,
      "estimated_cost": 0.0124
    }
  }
}
```

#### Authentication (Optional)
```bash
# Register
POST /api/v1/auth/register
{"username": "user", "password": "pass"}

# Login
POST /api/v1/auth/login
{"username": "user", "password": "pass"}

# Use token
POST /api/v1/research
Authorization: Bearer <token>
```

#### Health Check
```bash
GET /api/health
```

#### API Documentation
```bash
GET /api/docs  # Swagger UI
```

## üèóÔ∏è Architecture

### Agent Roles

1. **Researcher** - Gathers information (with web_search, calculator)
2. **Analyst** - Analyzes patterns (with calculator)
3. **Writer** - Synthesizes findings
4. **Critic** - Reviews and ensures quality

### Communication Flow

```
Task ‚Üí Dynamic Agent Selection
     ‚Üì
  Researcher (uses tools if needed)
     ‚Üì
  Analyst (analyzes data)
     ‚Üì
  Writer (creates document)
     ‚Üì
  Critic (reviews quality)
     ‚Üì
  Repeat until TERMINATE or max rounds
```

### Token Tracking Flow

```
Input ‚Üí Count tokens ‚Üí Agent processing
     ‚Üì
  Track output tokens
     ‚Üì
  Calculate cost
     ‚Üì
  Return (messages, stats)
```

## üîç Project Structure

```
learning-autogen/
‚îú‚îÄ‚îÄ src/autogen_research/
‚îÇ   ‚îú‚îÄ‚îÄ agents/              # Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ specialized_agents.py
‚îÇ   ‚îú‚îÄ‚îÄ teams/               # Team orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ research_team.py    # Unified v2.0 with all features
‚îÇ   ‚îú‚îÄ‚îÄ tools/               # NEW: Agent tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ web_search.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calculator.py
‚îÇ   ‚îú‚îÄ‚îÄ auth/                # NEW: JWT authentication
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jwt_auth.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokens.py        # NEW: Token counting
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observability.py # NEW: Sentry/OpenTelemetry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ database/            # Database models
‚îÇ   ‚îú‚îÄ‚îÄ tasks/               # Celery async tasks
‚îÇ   ‚îî‚îÄ‚îÄ models/              # LLM model factory
‚îú‚îÄ‚îÄ frontend/                # React UI
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ app.py                   # Flask API
‚îî‚îÄ‚îÄ migrations/              # Database migrations
```

## ‚öôÔ∏è Advanced Features

### Tool Registration

```python
from autogen_core.tools import FunctionTool

# Tools are automatically registered
team = ResearchTeam(enable_tools=True)

# Or register custom tools
def custom_tool(param: str) -> str:
    return f"Result: {param}"

tool = FunctionTool(custom_tool, description="Custom tool")
team.researcher.agent.register_tools([tool])
```

### Context Window Management

Automatically truncates conversation history to prevent context overflow:

```python
# Configurable in research_team.py
team.max_context_tokens = 4000  # Adjust based on model
```

### Observability

```bash
# Enable Sentry
export SENTRY_DSN=https://...@sentry.io/...

# Enable OpenTelemetry
docker run -d -p 4317:4317 jaegertracing/all-in-one
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
```

## üß™ Testing

```bash
# Run all tests
uv run pytest

# With coverage
uv run pytest --cov=src/autogen_research

# Type checking
uv run mypy src/

# Linting
uv run ruff check src/
```

## üö¢ Deployment

### Docker

```bash
docker-compose up -d
```

### Manual Deployment

```bash
# Start Redis
redis-server --daemonize yes

# Start Celery worker
celery -A src.autogen_research.tasks.celery_app worker -l info &

# Start app
gunicorn -k eventlet -w 1 -b 0.0.0.0:5001 app:app
```

## üìä Performance

### Speed Optimization

- Use `use_dynamic_routing=True` (30-50% faster)
- Reduce `MAX_ROUNDS` for simple tasks
- Use smaller models (llama3.2:1b)
- Switch to OpenAI for cloud speed

### Cost Optimization

- Use Ollama (free, local)
- Track costs per query with token stats
- Monitor `estimated_cost` in responses

### Typical Response Times

| Model | Time | Cost |
|-------|------|------|
| Ollama llama3.2 | 30-120s | $0.00 |
| Ollama llama3.2:1b | 15-60s | $0.00 |
| OpenAI GPT-4o-mini | 5-15s | ~$0.01 |
| OpenAI GPT-4 | 10-30s | ~$0.10 |

## üîß Troubleshooting

### Redis Connection Error
```bash
# Start Redis
redis-server
```

### Ollama Not Found
```bash
# Start Ollama
ollama serve

# Pull model
ollama pull llama3.2
```

### Token Count is 0
This is expected for Ollama (local models). Token counting works best with OpenAI models.

### Import Errors
```bash
# Reinstall dependencies
uv sync
```

## üìù Migration from v1.0

Version 2.0 is **100% backward compatible**. The only change is that `team.run()` now returns a tuple:

```python
# Old v1.0 (still works!)
team = ResearchTeam()
messages = team.run("task")  # Returns just messages

# New v2.0 (recommended)
messages, stats = team.run("task")  # Returns (messages, stats)

# Or ignore stats
messages, _ = team.run("task")
```

All new features are **opt-in** via parameters:
- `enable_tools=True` (tools enabled by default)
- `use_dynamic_routing=True` (enabled by default)

## üéØ What's New in v2.0

- ‚úÖ Tool calling (web_search, calculator)
- ‚úÖ Token tracking and cost estimation
- ‚úÖ Enhanced chain-of-thought prompts
- ‚úÖ Dynamic agent routing
- ‚úÖ Context window management
- ‚úÖ JWT authentication
- ‚úÖ Sentry & OpenTelemetry integration
- ‚úÖ Database performance indexes
- ‚úÖ Unified single ResearchTeam class

## üìÑ License

MIT License - see LICENSE file

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests
4. Submit a pull request

## üôè Acknowledgments

Built with:
- [AutoGen](https://github.com/microsoft/autogen) - Multi-agent framework
- [Ollama](https://ollama.ai/) - Local LLM runtime
- [Flask](https://flask.palletsprojects.com/) - Web framework
- [React](https://react.dev/) - Frontend UI
